{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News analysis\n",
    "### loss=warp, opt=sgd, agg=avg, constr=unitnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 0\n",
    "descr = 1\n",
    "def load_data(file: Path, delimiter=','):\n",
    "    with file.open('r') as fr:\n",
    "        for row in csv.reader(fr):\n",
    "            yield [row[title + 1], row[descr + 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(Path('2019_03_05_19_10_02_332991.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def tokenize(data):\n",
    "    bag_of_words = list()\n",
    "    for text_data in data:\n",
    "        res_tokenize = []\n",
    "        for el in text_data:\n",
    "            el_re = re.sub(r'[^\\w\\s]',' ',el)\n",
    "            el_words = el_re.lower().split()                  \n",
    "            res_tokenize.append([word for word in el_words if word not in stop_words])\n",
    "        bag_of_words.append(res_tokenize)\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenize(data)\n",
    "for text_data in data:\n",
    "    if not text_data[title] or not text_data[descr]:\n",
    "        data.remove(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['australia', 'pursue', 'tpp', 'minus', 'one'], ['nations', 'hope', 'move', 'forward', 'transpacific', 'partnership', 'trade', 'deal', 'america', 'withdraws']]\n"
     ]
    }
   ],
   "source": [
    "print(data[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabluary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for el in data:\n",
    "    words += el[title] + el[descr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=len(words))\n",
    "Tfidf_vect.fit(words)\n",
    "vocab = Tfidf_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21797\n"
     ]
    }
   ],
   "source": [
    "print(vocab['hope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, vocab):\n",
    "    news_ids = []\n",
    "    ids_= lambda t: [vocab.get(x) for x in t if vocab.get(x)]\n",
    "\n",
    "    return [[ids_(t[title]), ids_(t[descr])] for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(emb_size, vocab_size):\n",
    "    return np.random.normal(0, 1, size=(vocab_size, emb_size)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unitnorm():\n",
    "    for i in range(len(embs)):\n",
    "        norm = np.sqrt(sum(np.power(embs[i], 2)))\n",
    "        if (round(norm, 3) != 1):\n",
    "            #print(\"WARNING: l2 norm != 1! Let's norm\")\n",
    "            embs[i] = embs[i] / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(text_words):\n",
    "    return [embs[word] for word in text_words]\n",
    "\n",
    "def ids_to_vec(text_tuple):\n",
    "    return [np.average(get_emb(el), axis=0) for el in text_tuple]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sgd(idx, gradient, alpha):\n",
    "    embs[idx] -= alpha * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_warp(data, tuple_emb, sample_idx, gamma=1.0):\n",
    "    scalar = np.dot(tuple_emb[title], tuple_emb[descr])\n",
    "    scalar_random = 0\n",
    "    cache = {sample_idx}\n",
    "    N_max = 100\n",
    "    \n",
    "    N = 0\n",
    "    while (N < N_max):\n",
    "        num_rand = np.random.randint(len(data))\n",
    "        if num_rand not in cache:\n",
    "            sample_descr = data[num_rand][descr]\n",
    "            sample_descr_emb = ids_to_vec([sample_descr])[0]\n",
    "            cache.add(num_rand)\n",
    "            N += 1\n",
    "            scalar_random = np.dot(tuple_emb[title], sample_descr_emb)\n",
    "            res = 1 - scalar + scalar_random\n",
    "            if (res > 0):\n",
    "                break\n",
    "    if (N != 0):\n",
    "        k = int((N_max - 1) / N)\n",
    "        search_complex = sum([1/j for j in range(1, k)])\n",
    "        loss = search_complex * (gamma - scalar + scalar_random)\n",
    "        if (loss > 0):\n",
    "            return sample_descr, (search_complex*(sample_descr_emb - tuple_emb[descr]), search_complex*(-tuple_emb[title]), search_complex*tuple_emb[title])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(v1, v2):\n",
    "    return 1 - np.dot(v1, np.transpose(v2)) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(data, k = 10):\n",
    "    n_test = len(data)\n",
    "    recall = 0\n",
    "    descr_emb = [ids_to_vec([data[i][descr]]) for i in range(n_test)]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        if not data[i][title]:\n",
    "            continue\n",
    "\n",
    "        title_emb = ids_to_vec([data[i][title]])       \n",
    "        dist_arr = []\n",
    "        for i_1 in range(n_test):\n",
    "            dist_arr.append(get_dist(title_emb, descr_emb[i_1]))\n",
    "\n",
    "        sort_dist_arr = np.sort(dist_arr)\n",
    "        if dist_arr[i] <= sort_dist_arr[k-1]:\n",
    "            recall += 1\n",
    "    \n",
    "    return recall / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(data, tuple_title_descr, sample_idx, alpha, gamma):\n",
    "    embs_tuple = ids_to_vec(tuple_title_descr)\n",
    "    output_warp = backward_warp(data, embs_tuple, sample_idx, gamma)\n",
    "    if not output_warp:\n",
    "        return\n",
    "    \n",
    "    rand_descr, gradients = output_warp  \n",
    "    tuple_title_descr.append(rand_descr)   \n",
    "    \n",
    "    for i, g in zip(tuple_title_descr, gradients):\n",
    "        update_sgd(i, g, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(data, alpha, check_param, gamma):\n",
    "    first = np.random.permutation(len(data))\n",
    "    check_update = 0\n",
    "    for f in first:\n",
    "        u, v = data[f]\n",
    "        if u and v:\n",
    "            train2(data, [u, v], f, alpha, gamma)\n",
    "            check_update += 1\n",
    "            if check_update % check_param == 0:\n",
    "                unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, n_epochs, alpha, check_param, gamma):\n",
    "    for epoch in range(n_epochs):\n",
    "        t1 = time.process_time()\n",
    "        train1(train_data, alpha, check_param, gamma)\n",
    "        t = time.process_time() - t1\n",
    "        recall = recall_at_k(test_data)\n",
    "        print(\"Epoch {:>2} : recall = {:>2} time = {:>12}s\".\n",
    "              format(epoch, round(recall, 2), round(t, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 256\n",
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = text_to_id(data, vocab)  \n",
    "train_data, test_data1 = train_test_split(data_id, test_size=0.8)\n",
    "test_data, test_2 = train_test_split(data_id, test_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : recall = 0.92 time =     2375.047s\n",
      "Epoch  1 : recall = 0.89 time =     2314.047s\n",
      "Epoch  2 : recall = 0.9 time =     2364.641s\n",
      "Epoch  3 : recall = 0.88 time =     2257.812s\n",
      "Epoch  4 : recall = 0.88 time =     2298.656s\n",
      "Epoch  5 : recall = 0.89 time =     2284.359s\n",
      "Epoch  6 : recall = 0.89 time =     2334.016s\n",
      "Epoch  7 : recall = 0.91 time =     2262.406s\n",
      "Epoch  8 : recall = 0.9 time =     2331.062s\n",
      "Epoch  9 : recall = 0.89 time =     2319.781s\n"
     ]
    }
   ],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.05, check_param=30, gamma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding neighbor words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(query, k=10):\n",
    "    query_emb = embs[vocab.get(query)]\n",
    "    ind = vocab.get(query)\n",
    "    dist_arr = {}\n",
    "    \n",
    "    for i in range(0, len(embs)):\n",
    "        dist_arr[i] = get_dist(query_emb, embs[i])\n",
    "    \n",
    "    sorted_embs = sorted(dist_arr.items(), key=lambda kv: kv[1])[:k]\n",
    "    for emb in sorted_embs:\n",
    "        if emb[0] != ind:\n",
    "             print(\"\\t\" + dict(zip(vocab.values(), vocab.keys()))[emb[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tkilled\n",
      "\tkillings\n",
      "\tcrying\n",
      "\tthirty\n",
      "\tfreed\n",
      "\twoman\n",
      "\traid\n",
      "\tkill\n",
      "\teyewitnesses\n"
     ]
    }
   ],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvladimir\n",
      "\tkurdish\n",
      "\trussias\n",
      "\tsa\n",
      "\tbarbarism\n",
      "\tturkey\n",
      "\tfighters\n",
      "\tvigilante\n",
      "\tsyria\n"
     ]
    }
   ],
   "source": [
    "knn(\"putin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = text_to_id(data, vocab)  \n",
    "train_data, test_data = train_test_split(data_id, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : recall = 0.91 time =     9368.938s\n",
      "Epoch  1 : recall = 0.91 time =     9089.359s\n",
      "Epoch  2 : recall = 0.92 time =     9544.047s\n",
      "Epoch  3 : recall = 0.92 time =     9229.828s\n",
      "Epoch  4 : recall = 0.94 time =     9591.156s\n",
      "Epoch  5 : recall = 0.93 time =     9337.562s\n",
      "Epoch  6 : recall = 0.91 time =      9062.75s\n"
     ]
    }
   ],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.05, check_param=30, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha=0.01, gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : recall = 0.95 time =     4300.823s\n",
      "Epoch  1 : recall = 0.95 time =     2556.451s\n",
      "Epoch  2 : recall = 0.93 time =     2555.889s\n",
      "Epoch  3 : recall = 0.92 time =     2554.719s\n",
      "Epoch  4 : recall = 0.91 time =     2555.078s\n",
      "Epoch  5 : recall = 0.91 time =      2564.61s\n",
      "Epoch  6 : recall = 0.92 time =     2567.059s\n",
      "Epoch  7 : recall = 0.91 time =      2569.29s\n",
      "Epoch  8 : recall = 0.91 time =     2568.151s\n",
      "Epoch  9 : recall = 0.91 time =     2582.222s\n"
     ]
    }
   ],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.01, check_param=30, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdeadly\n",
      "\tothers\n",
      "\tinvolving\n",
      "\tkill\n",
      "\traids\n",
      "\tjailed\n",
      "\tkiller\n",
      "\tprovince\n",
      "\tinvestigating\n"
     ]
    }
   ],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha=0.05, gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : recall = 0.94 time =     6472.871s\n",
      "Epoch  1 : recall = 0.9 time =     2556.357s\n",
      "Epoch  2 : recall = 0.92 time =     2700.253s\n",
      "Epoch  3 : recall = 0.9 time =     2743.746s\n",
      "Epoch  4 : recall = 0.92 time =     2700.908s\n",
      "Epoch  5 : recall = 0.9 time =     2664.325s\n",
      "Epoch  6 : recall = 0.92 time =     2666.416s\n"
     ]
    }
   ],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.05, check_param=30, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha=0.01, gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.01, check_param=30, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(0, 1, size=(5, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
