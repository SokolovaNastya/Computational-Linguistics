{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss=warp, opt=sgd, agg=avg, constr=unitnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 0\n",
    "descr = 1\n",
    "def load_data(file: Path, delimiter=','):\n",
    "    with file.open('r') as fr:\n",
    "        for row in csv.reader(fr):\n",
    "            yield [row[title + 1], row[descr + 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(Path('../2019_03_05_19_10_02_332991.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def tokenize(data):\n",
    "    bag_of_words = list()\n",
    "    for text_data in data:\n",
    "        res_tokenize = []\n",
    "        for el in text_data:\n",
    "            el_re = re.sub(r'[^\\w\\s]',' ',el)\n",
    "            el_words = el_re.lower().split()                  \n",
    "            res_tokenize.append([word for word in el_words if word not in stop_words])\n",
    "        bag_of_words.append(res_tokenize)\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenize(data)\n",
    "for text_data in data:\n",
    "    if not text_data[title] or not text_data[descr]:\n",
    "        data.remove(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['australia', 'pursue', 'tpp', 'minus', 'one'], ['nations', 'hope', 'move', 'forward', 'transpacific', 'partnership', 'trade', 'deal', 'america', 'withdraws']]\n"
     ]
    }
   ],
   "source": [
    "print(data[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for el in data:\n",
    "    words += el[title] + el[descr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=len(words))\n",
    "Tfidf_vect.fit(words)\n",
    "vocab = Tfidf_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21797\n"
     ]
    }
   ],
   "source": [
    "print(vocab['hope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, vocab):\n",
    "    news_ids = []\n",
    "    ids_= lambda t: [vocab.get(x) for x in t if vocab.get(x)]\n",
    "\n",
    "    return [[ids_(t[title]), ids_(t[descr])] for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = text_to_id(data, vocab)  \n",
    "train_data, test_data1 = train_test_split(data_id, test_size=0.95)\n",
    "test_data, test_data2 = train_test_split(test_data1, test_size=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(emb_size, vocab_size):\n",
    "    eps = 0.001\n",
    "    return eps * np.random.randn(vocab_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unitnorm():\n",
    "    for i in range(len(embs)):\n",
    "        norm = np.sqrt(sum(np.power(embs[i], 2)))\n",
    "        if (round(norm, 3) != 1):\n",
    "            #print(\"WARNING: l2 norm != 1! Let's norm\")\n",
    "            embs[i] = embs[i] / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 1024\n",
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(text_words):\n",
    "    return [embs[word] for word in text_words]\n",
    "\n",
    "def ids_to_vec(text_tuple):\n",
    "    return [np.average(get_emb(el), axis=0) for el in text_tuple]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sgd(idx, gradient, alpha):\n",
    "    embs[idx, :] -= alpha * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_warp(data, tuple_emb, sample_idx, gamma = 1.0):\n",
    "    scalar = np.dot(tuple_emb[title], tuple_emb[descr])   \n",
    "    scalar_random = 0\n",
    "    sample_descr = []\n",
    "    cache = [sample_idx]\n",
    "    \n",
    "    N = 0\n",
    "    while (len(cache) < len(data)):\n",
    "        num_rand = np.random.randint(len(data))\n",
    "        if num_rand not in cache:\n",
    "            sample_descr = data[num_rand][descr]\n",
    "            sample_descr_emb = ids_to_vec([sample_descr])[0]\n",
    "            cache.append(num_rand)\n",
    "            N += 1\n",
    "            scalar_random = np.dot(tuple_emb[title], sample_descr_emb)\n",
    "            res = 1 - scalar + scalar_random\n",
    "            if (res > 0):\n",
    "                break\n",
    "    if (N != 0):\n",
    "        search_complex = sum([1/j for j in range(1,N + 1)])\n",
    "        loss = search_complex * (gamma - scalar + scalar_random)\n",
    "        if (loss > 0):\n",
    "            return sample_descr, (sample_descr_emb - tuple_emb[descr], (-1) * tuple_emb[title], tuple_emb[title])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k(vec, k):\n",
    "    return np.argsort(vec, axis=-1, kind='quicksort', order=None)[1:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(data, k = 10):\n",
    "    n_test = len(data)\n",
    "    descr_emb = [ids_to_vec([data[i][descr]]) for i in range(n_test)]\n",
    "    descr_embed = np.reshape(descr_emb, (feature_size, len(descr_emb)))\n",
    "    \n",
    "    recall = 0\n",
    "    N = 0\n",
    "    for i in range(n_test):\n",
    "        if not data[i][title]:\n",
    "            continue\n",
    "\n",
    "        title_emb = ids_to_vec([data[i][title]])\n",
    "        N += 1\n",
    "        if (i in top_k(np.matmul(title_emb, descr_embed)[0], k)):\n",
    "            recall += 1\n",
    "\n",
    "    return recall / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(data, tuple_title_descr, sample_idx, alpha):\n",
    "    embs_tuple = ids_to_vec(tuple_title_descr)\n",
    "    output_warp = backward_warp(data, embs_tuple, sample_idx)\n",
    "    if not output_warp:\n",
    "        return\n",
    "    \n",
    "    rand_descr, gradients = output_warp  \n",
    "    tuple_title_descr.append(rand_descr)   \n",
    "    \n",
    "    for i, g in zip(tuple_title_descr, gradients):\n",
    "        update_sgd(i, g, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(data, alpha, check_param):\n",
    "    first = np.random.permutation(len(data))\n",
    "    check_update = 0\n",
    "    for f in first:\n",
    "        u, v = data[f]\n",
    "        diff = lambda l1,l2: [x for x in l1 if x not in l2]\n",
    "        if (np.random.choice([True, False])):\n",
    "            v = diff(v, u)     \n",
    "        else:\n",
    "            u = diff(u, v) \n",
    "        if u and v:\n",
    "            train2(data, [u, v], f, alpha)\n",
    "            check_update += 1\n",
    "            if check_update % check_param == 0:\n",
    "                unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, n_epochs, alpha, check_param):\n",
    "    for epoch in range(n_epochs):\n",
    "        t1 = time.process_time()\n",
    "        train1(train_data, alpha, check_param)\n",
    "        t = time.process_time() - t1\n",
    "        recall = recall_at_k(test_data)\n",
    "        print(\"Epoch {:>2} : recall = {:>2}% time = {:>12}s\".\n",
    "              format(epoch, round(recall, 2), round(t, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8d448ef80a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-410a60771a90>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_data, test_data, n_epochs, alpha, check_param)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mtrain1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_at_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-7a8402a6a546>\u001b[0m in \u001b[0;36mtrain1\u001b[0;34m(data, alpha, check_param)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mcheck_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_update\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcheck_param\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0munitnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b08120910e3a>\u001b[0m in \u001b[0;36munitnorm\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munitnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;31m#print(\"WARNING: l2 norm != 1! Let's norm\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_data, test_data, 100, 0.05, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(query, k = 10):\n",
    "    embs_shape = np.reshape(embs, (feature_size, len(embs)))\n",
    "    scores = np.squeeze(np.matmul(embs[vocab.get(query)], embs_shape))\n",
    "    neighbours = np.argsort(scores, axis=-1, kind='quicksort', order=None)[1:k]\n",
    "    return [dict(zip(vocab.values(),vocab.keys()))[i] for i in neighbours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
