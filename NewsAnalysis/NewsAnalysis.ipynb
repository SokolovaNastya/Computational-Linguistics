{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News analysis\n",
    "### loss=warp, opt=sgd, agg=avg, constr=unitnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import spacy\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 0\n",
    "descr = 1\n",
    "def load_data(file: Path, delimiter=','):\n",
    "    with file.open('r') as fr:\n",
    "        for row in csv.reader(fr):\n",
    "            yield [row[title + 1], row[descr + 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(Path('2019_03_05_19_10_02_332991.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akharche\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def tokenize(data):\n",
    "    bag_of_words = list()\n",
    "    for text_data in data:\n",
    "        res_tokenize = []\n",
    "        for el in text_data:\n",
    "            el_re = re.sub(r'[^\\w\\s]',' ',el)\n",
    "            el_words = el_re.lower().split()                  \n",
    "            res_tokenize.append([word for word in el_words if word not in stop_words])\n",
    "        bag_of_words.append(res_tokenize)\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenize(data)\n",
    "for text_data in data:\n",
    "    if not text_data[title] or not text_data[descr]:\n",
    "        data.remove(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['australia', 'pursue', 'tpp', 'minus', 'one'], ['nations', 'hope', 'move', 'forward', 'transpacific', 'partnership', 'trade', 'deal', 'america', 'withdraws']]\n"
     ]
    }
   ],
   "source": [
    "print(data[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating vocabluary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for el in data:\n",
    "    words += el[title] + el[descr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=len(words))\n",
    "Tfidf_vect.fit(words)\n",
    "vocab = Tfidf_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21797\n"
     ]
    }
   ],
   "source": [
    "print(vocab['hope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, vocab):\n",
    "    news_ids = []\n",
    "    ids_= lambda t: [vocab.get(x) for x in t if vocab.get(x)]\n",
    "\n",
    "    return [[ids_(t[title]), ids_(t[descr])] for t in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(emb_size, vocab_size):\n",
    "    return np.random.normal(0, 1, size=(vocab_size, emb_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unitnorm():\n",
    "    for i in range(len(embs)):\n",
    "        norm = np.sqrt(sum(np.power(embs[i], 2)))\n",
    "        if (round(norm, 3) != 1):\n",
    "            #print(\"WARNING: l2 norm != 1! Let's norm\")\n",
    "            embs[i] = embs[i] / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(text_words):\n",
    "    return [embs[word] for word in text_words]\n",
    "\n",
    "def ids_to_vec(text_tuple):\n",
    "    return [np.average(get_emb(el), axis=0) for el in text_tuple]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sgd(idx, gradient, alpha):\n",
    "    embs[idx] -= alpha * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_warp(data, tuple_emb, sample_idx, gamma=1.0):\n",
    "    scalar = np.dot(tuple_emb[title], tuple_emb[descr])\n",
    "    scalar_random = 0\n",
    "    sample_descr = []\n",
    "    cache = {sample_idx}\n",
    "    N_max = 100\n",
    "    \n",
    "    N = 0\n",
    "    while (N < N_max):\n",
    "        num_rand = np.random.randint(len(data))\n",
    "        if num_rand not in cache:\n",
    "            sample_descr = data[num_rand][descr]\n",
    "            sample_descr_emb = ids_to_vec([sample_descr])[0]\n",
    "            cache.add(num_rand)\n",
    "            N += 1\n",
    "            scalar_random = np.dot(tuple_emb[title], sample_descr_emb)\n",
    "            res = 1 - scalar + scalar_random\n",
    "            if (res > 0):\n",
    "                break\n",
    "    if (N != 0):\n",
    "        search_complex = sum([1/j for j in range(1,N + 1)])\n",
    "        loss = search_complex * (gamma - scalar + scalar_random)\n",
    "        if (loss > 0):\n",
    "            return sample_descr, (sample_descr_emb - tuple_emb[descr], (-1) * tuple_emb[title], tuple_emb[title])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dist(v1, v2):\n",
    "    return 1 - np.dot(v1, np.transpose(v2)) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(data, k = 10):\n",
    "    n_test = len(data)\n",
    "    recall = 0\n",
    "    descr_emb = [ids_to_vec([data[i][descr]]) for i in range(n_test)]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        if not data[i][title]:\n",
    "            continue\n",
    "\n",
    "        title_emb = ids_to_vec([data[i][title]])       \n",
    "        dist_arr = []\n",
    "        for i_1 in range(n_test):\n",
    "            dist_arr.append(get_dist(title_emb, descr_emb[i_1]))\n",
    "\n",
    "        sort_dist_arr = np.sort(dist_arr)\n",
    "        if dist_arr[i] <= sort_dist_arr[k-1]:\n",
    "            recall += 1\n",
    "    \n",
    "    return recall / n_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(data, tuple_title_descr, sample_idx, alpha, gamma):\n",
    "    embs_tuple = ids_to_vec(tuple_title_descr)\n",
    "    output_warp = backward_warp(data, embs_tuple, sample_idx, gamma)\n",
    "    if not output_warp:\n",
    "        return\n",
    "    \n",
    "    rand_descr, gradients = output_warp  \n",
    "    tuple_title_descr.append(rand_descr)   \n",
    "    \n",
    "    for i, g in zip(tuple_title_descr, gradients):\n",
    "        update_sgd(i, g, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train1(data, alpha, check_param, gamma):\n",
    "    first = np.random.permutation(len(data))\n",
    "    check_update = 0\n",
    "    for f in first:\n",
    "        u, v = data[f]\n",
    "        if u and v:\n",
    "            train2(data, [u, v], f, alpha, gamma)\n",
    "            check_update += 1\n",
    "            if check_update % check_param == 0:\n",
    "                unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, test_data, n_epochs, alpha, check_param, gamma):\n",
    "    for epoch in range(n_epochs):\n",
    "        t1 = time.process_time()\n",
    "        train1(train_data, alpha, check_param, gamma)\n",
    "        t = time.process_time() - t1\n",
    "        recall = recall_at_k(test_data)\n",
    "        print(\"Epoch {:>2} : recall = {:>2} time = {:>12}s\".\n",
    "              format(epoch, round(recall, 2), round(t, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 256\n",
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = text_to_id(data, vocab)  \n",
    "train_data, test_data = train_test_split(data_id, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 : recall = 0.91% time =     8531.094s\n",
      "Epoch  1 : recall = 0.91% time =     8599.328s\n",
      "Epoch  2 : recall = 0.91% time =     8488.547s\n",
      "Epoch  3 : recall = 0.91% time =     8541.344s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8eae0dfa46e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-ab1aecf129e5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_data, test_data, n_epochs, alpha, check_param, gamma)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtrain1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         print(\"Epoch {:>2} : recall = {:>2}% time = {:>12}s\".\n\u001b[0;32m      8\u001b[0m               format(epoch, round(recall, 2), round(t, 3)))\n",
      "\u001b[1;32m<ipython-input-18-1b5539ead5c5>\u001b[0m in \u001b[0;36mrecall_at_k\u001b[1;34m(data, k)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mdist_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi_1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mdist_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescr_emb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msort_dist_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-8307b8035f24>\u001b[0m in \u001b[0;36mget_dist\u001b[1;34m(v1, v2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2433\u001b[0m     \"\"\"\n\u001b[1;32m-> 2434\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2436\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minexact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.05, check_param=30, gamma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding neighbor words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(query, k=10):\n",
    "    query_emb = embs[vocab.get(query)]\n",
    "    ind = vocab.get(query)\n",
    "    dist_arr = {}\n",
    "    \n",
    "    for i in range(0, len(embs)):\n",
    "        dist_arr[i] = get_dist(query_emb, embs[i])\n",
    "    \n",
    "    sorted_embs = sorted(dist_arr.items(), key=lambda kv: kv[1])[:k]\n",
    "    for emb in sorted_embs:\n",
    "        if emb[0] != ind:\n",
    "             print(\"\\t\" + dict(zip(vocab.values(), vocab.keys()))[emb[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tmurders\n",
      "\tattacking\n",
      "\tallegedly\n",
      "\tsent\n",
      "\tsuspected\n",
      "\ttexas\n",
      "\tshootings\n",
      "\tconvicted\n",
      "\tsuspects\n"
     ]
    }
   ],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tblaze\n",
      "\thours\n",
      "\tdestroyed\n",
      "\trescues\n",
      "\trescue\n",
      "\ttree\n",
      "\tevacuated\n",
      "\tcrashes\n",
      "\treopens\n"
     ]
    }
   ],
   "source": [
    "knn(\"fire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tobama\n",
      "\tbarack\n",
      "\tpresident\n",
      "\tallies\n",
      "\tdonald\n",
      "\ttrumps\n",
      "\twrites\n",
      "\texamines\n",
      "\thollande\n"
     ]
    }
   ],
   "source": [
    "knn(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tvladimir\n",
      "\trussias\n",
      "\tiran\n",
      "\tcountrys\n",
      "\tcuban\n",
      "\tiranian\n",
      "\trussian\n",
      "\tmaduro\n",
      "\tanticorruption\n"
     ]
    }
   ],
   "source": [
    "knn(\"putin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha=0.01, gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.01, check_param=30, gamma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha=0.05, gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.05, check_param=30, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alpha=0.01, gamma=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = create_embedding(feature_size, len(vocab))\n",
    "unitnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_data, test_data, n_epochs=10, alpha=0.01, check_param=30, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(\"weapon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
